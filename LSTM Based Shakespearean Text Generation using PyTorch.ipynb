{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "257c8641-ebc4-4180-bc11-a3e751dbdcf6",
   "metadata": {},
   "source": [
    "## LSTM Based Text Generation using PyTorch\n",
    "This is similar to what can be found in the text generation documentation by TensorFlow, but my focus is on implementing it using PyTorch, just to see how well it works on the framework. Moreover, they implemented it using GRU, while I'm working with an LSTM. <br><br>\n",
    "Link to the aforementioned documentation: <br>\n",
    "https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fe6c7c-68dd-4802-b605-d3859aeed15f",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efabd78e-e28a-4f05-b05e-9ad17990e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce103b34-3086-4ff6-bdc9-e2c54d77a3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01bda2f-779b-401c-bb7c-ad0c4eb314c4",
   "metadata": {},
   "source": [
    "### Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21304999-c234-4596-bef3-4ca53d632f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'shakespeare.txt'\n",
    "text = open(path, 'rb').read().decode(encoding = 'utf-8')\n",
    "text = text.replace('\\r', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eb895c1-d367-49f0-911c-1093044236e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 1115393\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "char_count = len(text)\n",
    "print('Number of characters:', char_count)\n",
    "print()\n",
    "print(text[0:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86daf898-14b1-4fb5-8d88-26ee20c03e93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# character vocabulary for prediction\n",
    "vocab = sorted(set(text))\n",
    "vocab_count = len(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4ad32-df0e-4304-a3b7-52622031505a",
   "metadata": {},
   "source": [
    "### Text processing\n",
    "1. Tokenize text at character level\n",
    "2. Create a text encoder and a decoder that reverses the encoding\n",
    "3. Join the decoded output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "726144b8-d526-42df-b4ac-0b0d06d906fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "def tokenize(text):\n",
    "    return np.array(list(text)) # string to list of character tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0635445-853a-4e7a-84ce-b04393ec4d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input:\tHello World!\n",
      "Encoded:\t[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42, 2]\n",
      "Decoded:\tHello World!\n"
     ]
    }
   ],
   "source": [
    "# text encoder & decoder \n",
    "s2i = {ch:i for i, ch in enumerate(vocab)} # {symbol : idx}\n",
    "i2s = {i:ch for i, ch in enumerate(vocab)} # {idx : symbol}\n",
    "\n",
    "encode = lambda s: [s2i[c] for c in s]\n",
    "decode = lambda l: ''.join([i2s[i] for i in l]) # join the output\n",
    "\n",
    "# testing\n",
    "sample = \"Hello World!\"\n",
    "enc = encode(sample)\n",
    "dec = decode(enc)\n",
    "print(f'Sample input:\\t{sample}\\nEncoded:\\t{enc}\\nDecoded:\\t{dec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a305846-bdd7-4140-b8a7-0018b4cf4aa2",
   "metadata": {},
   "source": [
    "### Setup for prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6788450d-8408-4c2f-802f-85c3cad68a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n',\n",
       "       ':', '\\n', 'B', 'e', 'f', 'o', 'r'], dtype='<U1')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the text...\n",
    "tokens = tokenize(text)\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "832c08c7-1c97-49f6-aa2d-8a6892a0e6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... and encode them\n",
    "all_ids = encode(text)\n",
    "all_ids[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a1deae5-4915-4a34-856c-90c369b76a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns: 1115293\n"
     ]
    }
   ],
   "source": [
    "# the idea is to have sequences of inputs where each input sequence will have a corresponding output which\n",
    "# is a single character existing right after that input sequence which starts in the next sequence.\n",
    "seq_len = 100\n",
    "input_text = []\n",
    "target_text = []\n",
    "for i in range(0, char_count - seq_len): # subract with seq_len to prevent out of range\n",
    "    seq_in = text[i:i + seq_len] # text[i + seq_len excluded]...\n",
    "    char_out = text[i + seq_len] # ...but included here.\n",
    "    input_text.append([s2i[char] for char in seq_in])\n",
    "    target_text.append(s2i[char_out])\n",
    "patterns = len(input_text)\n",
    "print(\"Total patterns:\", patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8be56f-8dc2-4f3d-8d33-f3a266920285",
   "metadata": {},
   "source": [
    "#### Few things to note:\n",
    "1. PyTorch's LSTM expects all of its inputs to be 3D tensors (sample, time steps, features).\n",
    "2. The data must be converted to floating point tensors.\n",
    "3. Normalizing the data helps training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aed71efe-c9d9-483e-815c-2d835daaf708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dimention: torch.Size([1115293, 100, 1])\n",
      "y dimention: torch.Size([1115293])\n"
     ]
    }
   ],
   "source": [
    "# data reshaping\n",
    "X = torch.tensor(input_text, dtype = torch.float32).reshape(patterns, seq_len, 1)\n",
    "X = X / float(vocab_count) # normalizing\n",
    "y = torch.tensor(target_text)\n",
    "print('X dimention:', X.shape)\n",
    "print('y dimention:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dd063a-3dcd-435d-9772-f54ddea069a1",
   "metadata": {},
   "source": [
    "### Model defining & training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668feeda-65cf-41dd-a3ff-45027b69fa4f",
   "metadata": {},
   "source": [
    "**Hidden states** are basically a function of the current and previous inputs. It evolves with information at each time step.<br>\n",
    "**Logits** are raw outputs from a network. They are the unscaled scores for each class in a classification task.<br>\n",
    "**Cross-entropy loss** is to be used as we are predicting a single class from 65 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cfca82e-abf6-4bf5-9f02-266740b6dcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─LSTM: 1-1                              1,844,224\n",
      "├─Dropout: 1-2                           --\n",
      "├─Linear: 1-3                            16,705\n",
      "=================================================================\n",
      "Total params: 1,860,929\n",
      "Trainable params: 1,860,929\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# model definition\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size = 1,\n",
    "                            hidden_size = 256,\n",
    "                            num_layers = 4,\n",
    "                            batch_first = True,\n",
    "                            dropout = 0.225) # 4 lstm layers with 256 hidden units each\n",
    "        self.dropout = nn.Dropout(0.225)\n",
    "        self.linear = nn.Linear(256, vocab_count) # dense layer\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x) # will look into 'hidden' later\n",
    "        x = x[:, -1, :] # only the last time step is taken as it contains the most information\n",
    "        x = self.linear(self.dropout(x)) # produce output\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = Model().to(device)\n",
    "epochs = 40\n",
    "batch_size = 128\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_function = nn.CrossEntropyLoss(reduction = 'sum')\n",
    "loader = data.DataLoader(data.TensorDataset(X, y),\n",
    "                         shuffle = True,\n",
    "                         batch_size = batch_size) \n",
    "\n",
    "# model summary\n",
    "print(summary(model, verbose = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f803fbab-c6fd-4c40-9340-bb37ff53b552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Cross-entropy: 2421611.5000\n",
      "Epoch 2: Cross-entropy: 2113608.2500\n",
      "Epoch 3: Cross-entropy: 1964630.8750\n",
      "Epoch 4: Cross-entropy: 1867911.8750\n",
      "Epoch 5: Cross-entropy: 1786178.3750\n",
      "Epoch 6: Cross-entropy: 1740098.3750\n",
      "Epoch 7: Cross-entropy: 1710207.8750\n",
      "Epoch 8: Cross-entropy: 1680417.5000\n",
      "Epoch 9: Cross-entropy: 1644015.3750\n",
      "Epoch 10: Cross-entropy: 1622073.1250\n",
      "Epoch 11: Cross-entropy: 1643736.5000\n",
      "Epoch 12: Cross-entropy: 1579415.5000\n",
      "Epoch 13: Cross-entropy: 1569533.2500\n",
      "Epoch 14: Cross-entropy: 1556315.3750\n",
      "Epoch 15: Cross-entropy: 1540297.5000\n",
      "Epoch 16: Cross-entropy: 1536158.7500\n",
      "Epoch 17: Cross-entropy: 1516412.6250\n",
      "Epoch 18: Cross-entropy: 1516249.1250\n",
      "Epoch 19: Cross-entropy: 1503287.2500\n",
      "Epoch 20: Cross-entropy: 1495808.6250\n",
      "Epoch 21: Cross-entropy: 1495943.6250\n",
      "Epoch 22: Cross-entropy: 1484764.7500\n",
      "Epoch 23: Cross-entropy: 1483875.7500\n",
      "Epoch 24: Cross-entropy: 1471096.0000\n",
      "Epoch 25: Cross-entropy: 1471430.5000\n",
      "Epoch 26: Cross-entropy: 1465601.7500\n",
      "Epoch 27: Cross-entropy: 1465471.2500\n",
      "Epoch 28: Cross-entropy: 1464496.8750\n",
      "Epoch 29: Cross-entropy: 1468690.0000\n",
      "Epoch 30: Cross-entropy: 1447324.7500\n",
      "Epoch 31: Cross-entropy: 1454104.7500\n",
      "Epoch 32: Cross-entropy: 1437087.8750\n",
      "Epoch 33: Cross-entropy: 1437828.7500\n",
      "Epoch 34: Cross-entropy: 1432334.8750\n",
      "Epoch 35: Cross-entropy: 1429439.0000\n",
      "Epoch 36: Cross-entropy: 1431387.3750\n",
      "Epoch 37: Cross-entropy: 1427663.5000\n",
      "Epoch 38: Cross-entropy: 1422193.8750\n",
      "Epoch 39: Cross-entropy: 1420745.6250\n",
      "Epoch 40: Cross-entropy: 1419062.8750\n",
      "\n",
      "Elapsed time: 279.76 minutes\n"
     ]
    }
   ],
   "source": [
    "# now we train the model!\n",
    "best_model = None\n",
    "best_loss = np.inf\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        # forward pass\n",
    "        y_pred = model(X_batch.to(device)) \n",
    "        # loss computation\n",
    "        loss = loss_function(y_pred, y_batch.to(device))\n",
    "        # backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # validation\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad(): # gradients not required for validation\n",
    "        for X_batch, y_batch in loader: \n",
    "            y_pred = model(X_batch.to(device)) \n",
    "            loss += loss_function(y_pred, y_batch.to(device))\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = model.state_dict() \n",
    "        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch + 1, loss))\n",
    "end = time.time()\n",
    "\n",
    "torch.save([best_model, s2i], \"single-char.pth\") # saving the mappings (s2i) as well for later use\n",
    "\n",
    "elapsed_time = end - start\n",
    "print()\n",
    "print(\"Elapsed time: {:.2f} minutes\".format(elapsed_time / 60)) # just curious..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044d9397-94c4-46a0-9620-fb8f69880f06",
   "metadata": {},
   "source": [
    "### Generating text\n",
    "Now that we have a trained model, lets generate some text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ecc30f9-5875-4b7d-a487-3acd7aafb262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the trained model\n",
    "best_model, s2i = torch.load(\"single-char.pth\", weights_only = True)\n",
    "vocab_count = len(s2i)\n",
    "i2s = dict((i, s) for s, i in s2i.items()) # {idx : symbol}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "241ed78d-7edc-4101-baf5-4a1fd40c627f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the model\n",
    "model = Model().to(device)\n",
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ad8d808-117c-45ef-bec3-d442a0ae619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i chose a random section from the text as a prompt\n",
    "prompt = '''ROMEO:\n",
    "Tut, I have lost myself; I am not here;\n",
    "This is not Romeo, he's some other where.\n",
    "\n",
    "BENVOLIO:\n",
    "Tell me in sadness, who is that you love.\n",
    "\n",
    "ROMEO:\n",
    "What, shall I groan and tell thee?'''\n",
    "\n",
    "prompt_enc = encode(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13796459-2643-48dd-b18c-e7e960066293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: ROMEO:\n",
      "Tut, I have lost myself; I am not here;\n",
      "This is not Romeo, he's some other where.\n",
      "\n",
      "BENVOLIO:\n",
      "Tell me in sadness, who is that you love.\n",
      "\n",
      "ROMEO:\n",
      "What, shall I groan and tell thee?\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Generation:\n",
      "all be heard the world of the company.\n",
      "\n",
      "KING RICHARD III:\n",
      "And he is not the soul of the world to hear,\n",
      "And so make her that have the common hand,\n",
      "That thou shalt thought the body of the common showers\n",
      "That shall be so a man to the man of soul.\n",
      "\n",
      "LING RICHARD III:\n",
      "The care and the state of the death of thee,\n",
      "The sea and hand of his son might be there,\n",
      "The strength of soldiers with the prince the life\n",
      "That should be so dead to the common courtesy,\n",
      "And he hath been a thousand things to thee,\n",
      "And then the sun of such a shame and life,\n",
      "I will not show the house of Rome, and then\n",
      "And then I have been so beheld the prince,\n",
      "That shall be so the house of her of her\n",
      "That be the soul of some of the warlike son.\n",
      "\n",
      "BUCKINGHAM:\n",
      "The world of the father's prince of York and shall\n",
      "The man that shall be such a brother here,\n",
      "The prince of love is so the world of her.\n",
      "\n",
      "KING RICHARD III:\n",
      "The sight and heart of the thing the son of him,\n",
      "That should be a son of his country's head.\n",
      "\n",
      "BUCKINGHAM:\n",
      "The prince the l\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print('Prompt:', prompt)\n",
    "print()\n",
    "print('--------------------------------------------------')\n",
    "print()\n",
    "print('Generation:')\n",
    "batch_size = 1 \n",
    "with torch.no_grad():\n",
    "    for i in range(1000):\n",
    "        # format the prompt as the model expects a vector\n",
    "        x = np.reshape(prompt_enc, (1, len(prompt_enc), 1)) / float(vocab_count)\n",
    "        x = torch.tensor(x, dtype = torch.float32).to(device)\n",
    "        prediction = model(x) # logits\n",
    "        temperature = 1.1 # adjust the softmax distribution (> 1: more creative, < 1: more deterministic)\n",
    "        probabilities = torch.softmax(prediction / temperature, dim = -1)\n",
    "        samples = [torch.multinomial(probabilities, num_samples = 1).item() for _ in range(80)]\n",
    "        idx = max(set(samples), key = samples.count) # takes the most frequent sample from 80 samples\n",
    "        # idx = torch.multinomial(probabilities, num_samples = 1).item()\n",
    "        result = decode([idx])\n",
    "        # print(result)\n",
    "        print(result, end = \"\")\n",
    "        # append new character to prompt for next iteration\n",
    "        prompt_enc.append(idx)\n",
    "        prompt_enc = prompt_enc[1:] # shift right\n",
    "print('\\n\\nDone!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f437d5c-f915-4ae9-b45d-89e55cea790f",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Even though the results aren't perfect, it's truly fascinating to see how a model that was trained to generate text at a character level, can produce words and sometimes even sentences that make sense. Even though we can dive deep into the mathematics and algorithms behind these networks, the real question still remains: how do these simple numbers achieve such complex results?<br>\n",
    "\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
